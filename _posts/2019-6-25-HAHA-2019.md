---
layout: post
title: HAHA 2019 Competition
---

When this image of a tweet showed up in the [Fastai forum](https://forums.fast.ai/t/nlp-challenge-project/44153), I had to give it a shot.

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.png)<!-- .element height="50%" width="50%" -->

I have been tinkering around with sequence models of various kinds for a few years and this challenge was a great way to refine my knowledge and understanding of their application.  From the posts in the forum, the game plan seemed straight forward;  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

I came in 3rd in classification and 2nd in the regression task.  The top three teams all used some version of a pre-trained lanauge model (LM) with a new "head" and some kind of ensemble to pick a model for the test set.  Kevin Bird and Hiromi Suenaga explain their layout their [2nd place entry here](http://kevinbird15.com/2019/06/26/High-Level-Haha-Architecture.html).

# The Fast.ai Forum Community
For almost a year now, I have been looking around on the fast.ai forums(link), helping to answer qustions and submitting pull requests (PRs) to the core library(contributor link?).  The `text` part of the library seemed particularly interesting to me so I dove in deep.

My journey started by replicating the Wiki103 pre-training to IMDB classification pipeline(link).  I pulled the data and replicated the process but swapped out the SpaCy tokenizer(link) for sentencepiece(link).  From there, I had a full process baseline and developed an understanding of how all the parts all worked.  As has been said many times in the forums, you don't learn from watching the videos and reading the notebooks - you learn from building it all yourself.  

# No Habla Espa&ntilde;ol 
With the details of the competition and an understanding of my intended process I started to work through the data and building my classification pipeline.  I don't speak Spanish but I viewed this as an advantage since I could not engineer features.  I had to apply the process and look at the metrics rather than trying to engineer a solution; The network had to find the answers.  I followed method of transfer-learning from lecture X (link) that is the basis of Universal Langage Model Fine-tuning for Text Classification (ULMFiT) (link).

1. Pretrain a language model (LM) with a large corpus of text
2. Fine-tune that LM on the text data for the competition
3. Train a classifier on top of the back-bone LM

The whole process went in a loop with a selected LM design and corpus all the way to classification of the validation data set and then back to improving the LM training data and the LM model.  

# Building a Language Model
A language model (LM) embeds the structure of the lanugage into the neural network.  The architecture of this model becomes the "backbone" for the classifer you will evenutally use.  I experiemnted with several, but in the end I used the AWD_LSTM model with QRNN units as defined in fast.ai.  
```python
config = awd_lstm_lm_config.copy()
config['qrnn'] = True
config['n_hid'] = 2304
learn = language_model_learner(data, AWD_LSTM, drop_mult=dropmult, pretrained=False, config=config)
```
To train this LM, you use the prior information in the sequence of tokens to predict the next token.  Often you have one token per word, but sometimes you can have sub-word units and you are predicting the next sub-word unit in a sequence.  A sub-word units woudl be `predict` and `ing`.  There are many words than end in `ing` so it is likely that `ing` is a sub-word unit in your vocabulary in english.  
Whatever you chose for your LM architecture you need a corpus for training the LM and you want the vocabulary of that corpus to map onto your classifier training set as best as possible so that you don't have words that are outside your stated vocabulary.  

# Buliding a Classifier
With a backbone defined, you now change the top of the model and can either create a classifier or a regression head.  For the classifier, I had a label for all my data training and validation data but the classes were imbalanced.  I used Synthetic Minority Oversampling Technique (SMOTE) to balance.  I have found from other projects that this can help the model train much better than feeding in the imbalanced cases. 
For the regression, I only had data for the "humorous" tweets.  I filled in the data for the non-humours tweets with zeros since I wanted them to get a low rating.

# Selecting the LM Training Corpus
This process was iterative.  I made a first attempt, tried to classify the data, then went back and improved my training corpus and continued to refine my LM to improve my model. 

## First Attempt: Wikipeida
I started with Spanish Wikipedia and tried to train an LM from there.  Since I had a pipleine using sentencepiece, I used that for my tests. I took that LM and built a classifier for tweets from the competition (with no fine-tuning) and saw a lot of out of vocabulary tokens. That meant that the vocabulary coverage from Spanish Wikipedia did not overlap well with the Twitter vocabulary.

Even without speaking Spanish, I could see why:
>Jajajajajajaja xque todos me contestan ese motivo, tan obvio es todo (?😂😂😂😂

>Mi Espalda Me Quiere Matarrrrrrr😭😭

>¡FELIZ NAVIDAD A TODOS! 🎄🎅🏻

>#FiebreDeViña Bose tiene cara de muerto, o se cree vampiro🎧

There are a lot of emoticons!  There is a bunch of slang and other words that would not show up in Wikipedia entries.  It was time to find a new LM corpus. 

## Second Attempt: Spanish Twitter
About a year ago, I had gotten fed up with how Twitter moved around the time sequence of my feed and tried to show me what I might like.  So, I went to look for a command line twitter feed app and found tweepy (link) and adapted that to my needs.  
To collect a corpus for my LM, I pulled any tweet marked as spanish with the terms 'el','su','lo','y' or 'en'.  I excluded re-tweets and put that all into a file.  I ran the script over three 4-hour periods and got 475k tweets to train on.  
When I used this new LM to train my classifier, I had less out of vocab words and I was able to use the emoticons as part of my classification.  The resulting classifer did better than the one trained on Wikipedia. So I proceeded with other improvements.

# Fine-tuning the model.
With my LM model corpus set I fine-tuned the weights of the LM by looking at the train _and test_ text data. I did nto expand or change the vocabulary from what I had in my oringal model, but I did finetune the LM so that it best predicts the next-word in the datset we are going to use for classification.  This enhancement further boosted the performance of the resulting classifer.  Again, reinforcing the idea that a better LM leads to a better final model.  

# Two more enhancements
## Cross-validation 
As the deadline for the competion neared, the more people posted on the forum thread.  The first-place entrant talked about doing 5-fold cross validation.  Up until this point, I had a single validation set and training set.  With that kind of training, you are bound to over-fit for _that_ particular validation set.  With cross-validation, you will select 5 different validation sets and then can average the model ouputs to create a model that is more robust over the futrue test data you will see.

## Setting the seed
In working code for the core library, I had found setting the random seed (link stas?) to be very helpful. I wanted results that were identical so I could test out the code changes I was making. In talking with Piotr on the forums, he suggested using the seed as a hyper-parameter and sampling over many seeds to see which one would produce the best classification output.  I combined this with my cross-validation and tested out 20 random seeds with 5-fold cross-validation on each one.  The result was 20 robust models and I selected the best for my competition submission.   

# Conclusion
Being in a live competition pushed me to learn much more than I had just looking at notebooks and replicating outputs from scratch.  when I fell from first place to second, I was _really_ motivated to start to figure out other details and tweaks to improve my score.  I learned a lot but more than that it was the contribution, suggestions and encouragement from the community that pulled me along to the finish line.  

***
# Lessons Learned:

link to KevinBird blog about the same: http://kevinbird15.com/2019/06/11/Haha-2019-Lessons-Learned.html

Lessons: 
* You learn a lot when pushed by a compeition.  You are critical of your process and you look into all the details to extract and edge.
* The community is a huge help. Put your code out there, people give feedback and want to assist you and help you out.  You get lots of suggestions when you have an example out there.
* Writing up your results and consolatating your code gives you ideas about what makes sense to try/do next and to improve on your model.  When I moved the code into a clean repo and started to document what I had done, I realized I had missed the fine-tuning stage. 
* GIT is your friend.  I came to love it and to use it to see what I had changed/used at each stage of the compitition and then revert back to points where things had run better in my final results.  It really did help me to stay organized during development. 
* Pre-training and transfer learning is a big deal - it works and is worth your time to figure it out in NLP.  You can embbed a lot of information in the network with pre-training.  The winner used pre-trained BERT which is a huge amount of information embedded into the model. 
* Controlling the random seed and using it as a hyperparameter.  Great idea/suggestion.  It really did help me to stay sane and have results that I could replicate when I wanted to go back and re-build a model.  I set the random seed and got the exact same results. 



***
Outline:
1. Saw challenge on forum/Twitter - sounded cool and fun.

Random Seed control / Random seed as a hyper parameter

2. Scraped data from Twitter to use for LM training from scratch
?    2a. Already had SP pipeline for Wiki103 from scratch.  Used again with BPE model for sub-word units
3. Trained model and built classifer - got decent result better than NBSVM
4. Moved over/wrote up results and realized I had _missed_ the fine-tuning step.  So added that back in.

4. Reinfed the model with
    4a. Increased dropout and WD on LM fit to improve generalization
    4b. Added label smoothing to loss function - allowed me to unfreeze all for classifier and fit
    4c. Finetune of the LM on the classification text data (particular task)
    4d. started with 100 random seed init, same validation set for control
    4e. Added cross-validation of results for ensemble - does it work in different validation sub-sets?
        4e i. Run this 5-fold over 20 random seeds (100 tests, but average over each seed.)
    
Tried but did not work:
* Ensemble Forward and Backward classification.  backward was consistently worse, so averaging just reduced the power of the classifier
* Mixup - did mixup after the embedding step.  But that did not seem to help much with the output results.
~~* Started on UDA implemetnation of NMT - but did not get working in time.~~

    

#  Advantages of a Competition Environment

After my first few submissions, I was in first place and became complacent.  I had posted to the forums and had gotten some feedback but had not heard much.  Then, a week before the compeition closed, I got passed by a large margin. What is going on here?  I started to dig in more and it was that competition that really drove me to learn more, try more and do more than I had done for the month before.  

There are several places on the Fast.ai forums where people talk about learning from doing competitions.  I was suspicous of that, but after this experience I can see what they mean.  Having a deadline and having competitors and collaborators really drives you to understand and do more than you would on your own and with your own time. 

