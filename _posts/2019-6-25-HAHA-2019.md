---
layout: post
title: HAHA 2019 Competition
---

When this image of a tweet showed up in the [Fastai forum](https://forums.fast.ai/t/nlp-challenge-project/44153), I had to give it a shot.

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.png)<!-- .element height="50%" width="50%" -->

I have been tinkering around with sequence models of various kinds for a few years now and challenge was a great way to refine my knowledge and understanding of their application.  From the posts in the forum, the game plan seemed straight forward;  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

I came in 3rd in classification and 2nd in the regression task.  The top three teams all used some version of a pre-trained lanauge model (LM), a new "head" some kind of ensemble to pick a model for the test set.  

# The Fast.ai Forum Community 

For almost a year now, I have been looking around on the fast.ai forums(link), helping to answer qustions submitting pull requests (PRs) to the core library(contributor link?).  I started with makeing contributions to the documentation working my way up from there.  Starting with documentation was a great way to learn about git, the PR process and the fast.ai style.  The `text` part of the library seemed particularly interesting to me so I dove in deep.

My journey started by replicating the Wiki103 pre-training to IMDB classification pipeline(link).  I did it all from scratch and swapped out the SpaCy tokenizer(link) for sentencepiece(link).  From there, I had a full process baseline and developed an understanding of how the parts all worked.  As has been said many times in the forums(link), you don't learn from watching the videos and reading the notebooks - you learn from building it all yourself.  

# No Habla Espa&ntilde;ol 

With some baseline understanding and the details of the compeition set out I looked at the data.  I don't speak Spanish but I viewed this as an advantage.  Since I could not engineer features I had to apply the process and believe that the process would sort out the details. I followed method of transfer-learning from lecture X (link) that is the basis of Universal Langage Model Fine-tuning for Text Classification (ULMFiT) (link).

1. Pretrain a language model (LM) with a large corpus of text
2. Fine-tune that LM on the text data for the competition
3. Train a classifier on top of the back-bone LM

# Selecting the LM Training Corpus
## First Attempt: Wikipeida
I started with Spanish Wikipedia and tried to train an LM from there.  Since I had a pipleine using sentencepiece, I used that for my tests. I took that and went to classifier the tweets from the competition (with no fine-tuning) and saw a lot of out of vocabulary tokens. That meant that the vocabulary coverage from Spanish Wikipedia did not overlap well with the Twitter vocabulary.

Looking at a couple examples, it became clear why:
```
```
There are a lot of emoticons!  There is a bunch of slang and other words that woudl not show up in Wikipedia entries.  It was time to find a new LM corpus. 

## Second Attempt: Spanish Twitter
About a year ago, I had gotten fed up with how Twitter moved around the time sequence of my feed and tried to show me what I might like.  I went to look for a command line twitter feed and could not find one but did come across tweepy (link).  I already had code to track my own twitter feed and used to to pull any tweet marked as spanish with the terms 'el','su','lo','y' or 'en'.  I excluded re-tweets and put that all into a file.  I ran the script over three 4-hour periods and got 475k tweets to train on.  

When I used this new LM to train my classifier, I had less out of vocab words and I was able to use the emoticons as part of my classification.  Now I was ready to refine the classifier.


# Buliding a Classifier


***

(include?, separate posts?  just talk about SP and how it works or is used?)
## Sentencepice tokenization to sub-word units
Sentencepice tokenizes into sub-word units (link) and as with all tokenizers, you pick a vocab size cutoff.  When you parse into words, there is often a long-tail of infrequent words in your training set that you excluded from your vocabulary.  

With sentencepiece, you break words up into sub-word units all the way down to single characters.  This means that you could build up a word from the sub-word units and have more coverage and less out of vocabulary words.  

# Lessons Learned:

link to KevinBird blog about the same: http://kevinbird15.com/2019/06/11/Haha-2019-Lessons-Learned.html

Lessons: 
* You learn a lot when pushed by a compeition.  You are critical of your process and you look into all the details to extract and edge.
* The community is a huge help. Put your code out there, people give feedback and want to assist you and help you out.  You get lots of suggestions when you have an example out there.
* Writing up your results and consolatating your code gives you ideas about what makes sense to try/do next and to improve on your model.  When I moved the code into a clean repo and started to document what I had done, I realized I had missed the fine-tuning stage. 
* GIT is your friend.  I came to love it and to use it to see what I had changed/used at each stage of the compitition and then revert back to points where things had run better in my final results.  It really did help me to stay organized during development. 
* Pre-training and transfer learning is a big deal - it works and is worth your time to figure it out in NLP.  You can embbed a lot of information in the network with pre-training.  The winner used pre-trained BERT which is a huge amount of information embedded into the model. 
* Controlling the random seed and using it as a hyperparameter.  Great idea/suggestion.  It really did help me to stay sane and have results that I could replicate when I wanted to go back and re-build a model.  I set the random seed and got the exact same results. 



***
Outline:
1. Saw challenge on forum/Twitter - sounded cool and fun.

Random Seed control / Random seed as a hyper parameter

2. Scraped data from Twitter to use for LM training from scratch
?    2a. Already had SP pipeline for Wiki103 from scratch.  Used again with BPE model for sub-word units
3. Trained model and built classifer - got decent result better than NBSVM
4. Moved over/wrote up results and realized I had _missed_ the fine-tuning step.  So added that back in.

4. Reinfed the model with
    4a. Increased dropout and WD on LM fit to improve generalization
    4b. Added label smoothing to loss function - allowed me to unfreeze all for classifier and fit
    4c. Finetune of the LM on the classification text data (particular task)
    4d. started with 100 random seed init, same validation set for control
    4e. Added cross-validation of results for ensemble - does it work in different validation sub-sets?
        4e i. Run this 5-fold over 20 random seeds (100 tests, but average over each seed.)
    
Tried but did not work:
* Ensemble Forward and Backward classification.  backward was consistently worse, so averaging just reduced the power of the classifier
* Mixup - did mixup after the embedding step.  But that did not seem to help much with the output results.
~~* Started on UDA implemetnation of NMT - but did not get working in time.~~

    

#  Advantages of a Competition Environment

After my first few submissions, I was in first place and became complacent.  I had posted to the forums and had gotten some feedback but had not heard much.  Then, a week before the compeition closed, I got passed by a large margin. What is going on here?  I started to dig in more and it was that competition that really drove me to learn more, try more and do more than I had done for the month before.  

There are several places on the Fast.ai forums where people talk about learning from doing competitions.  I was suspicous of that, but after this experience I can see what they mean.  Having a deadline and having competitors and collaborators really drives you to understand and do more than you would on your own and with your own time. 

# Building a Lanauge Model

The most important things I learned, it was that the quality of the pre-trained Langugae Model (LM) defines your ability to classify well.  An LM is the backbone of of your classifer or regresion model - so you have to get it right.

When you think about an LM, think about predicing the next token in a sequence.  That token can be a word, it can be a sub-word unit, it can be an n-gram of words.  But in the end, you are taking the information up to time t and predicting the next token at time t+1.  I toyed around with several kinds of LMs, but in the end chose the AWD_LSTM to train and apply to this problem.

# Spanish Twitter

When training and LM, you want to use data that is broad and deep in your area of application.  Typically for English, people use the Wikipedia corpus.  When processing this corpus and reducing the data down to a vocabulary of 60,000 words, you find that you drop a lot of less common words and phrases.  If your classification model looks like your training corpus you are all set. If it does not, you are going to get a lot of words that are out-of-vocab (OOV).

The problem with OOV is that you will label those as unknown (in my case, `xxunk` is the token that gets put in there.)  This can be a big gotcha for the LM and lead to false information.  Take the extreme example - all your words are OOV. Now you are going to get `xxunk xxunk xxunk...` which means that no matter what you think you see in a sequence you know the next term you are going to see - `xxunk`!!

To aleivate this problem I did two things
1.  Build a corpus of words from Twitter by sampling the data on a live stream over 3 4-hour sessions.
1.  Use Sentencepiece to break words into sub-word units so that I got a lot of "parts" of words and had less OOV terms when I saw things in my test set.

# Fine Tuning

Spanish twitter is pretty broad.  But for the HAHA compeitition, there is a particular group of terms that we encounter.  So, we can "fine-tune" our LM with the data.  We can take the text part of the train *and test* data and combine it into our fine-tune corpus. Then we can train our LM on that data and get a final LM which can best predict our next-word in the text we expect to see.  For this model, we got about 31% accuracy when predicting the next token.

# Build the classifier or regression model
