---
layout: post
title: HAHA 2019 Competition
---

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.jpg)

When this showed up in the Fastai forum, I had to give it a shot.  I have been tinkering around with sequence models of various kinds for a few years now and this seemed like the perfect forum.  The game plan seemed straight forward.  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

In the final competition, I came in 3rd in classification and 2nd in the regression task.  All the top competitors used a modern-NLP approach with a pre-trained lanauge model (LM), a new "head" some kind of ensemble to pick a model for the test set.  I learned a lot, and built a lot and wanted to share more of that here.

# Building a Lanauge Model

A language model (LM) is the backbone of training in NLP.  There are many ways to build a model, many structures to use and many (small) choices to make along the way. If there is just one thing I learned, it was that the quality of the pre-trained LM defines your ability to classify well.    
