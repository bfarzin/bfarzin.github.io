---
layout: post
title: HAHA 2019 Competition
---

When this image of a tweet showed up in the [Fastai forum](https://forums.fast.ai/t/nlp-challenge-project/44153), I had to give it a shot.

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.png)<!-- .element height="50%" width="50%" -->

I have been tinkering around with sequence models of various kinds for a few years and this challenge was a great way to refine my knowledge and understanding of their application.  From the posts in the forum, the game plan seemed straight forward;  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

I came in 3rd in classification and 2nd in the regression task.  The top three teams all used some version of a pre-trained lanauge model (LM) with a new "head" and some kind of ensemble to pick a model for the test set.  Kevin Bird and Hiromi Suenaga explain their layout their [2nd place entry here](http://kevinbird15.com/2019/06/26/High-Level-Haha-Architecture.html).

# The Fast.ai Forum Community

For almost a year now, I have been looking around on the fast.ai forums(link), helping to answer qustions and submitting pull requests (PRs) to the core library(contributor link?).  The `text` part of the library seemed particularly interesting to me so I dove in deep.

My journey started by replicating the Wiki103 pre-training to IMDB classification pipeline(link).  I pulled the data and replicated the process but swapped out the SpaCy tokenizer(link) for sentencepiece(link).  From there, I had a full process baseline and developed an understanding of how all the parts all worked.  As has been said many times in the forums, you don't learn from watching the videos and reading the notebooks - you learn from building it all yourself.  

# No Habla Espa&ntilde;ol 

With the details of the competition and an understanding of my intended process I started to work through the data and building my classification pipeline.  I don't speak Spanish but I viewed this as an advantage since I could not engineer features.  I had to apply the process and look at the metrics rather than trying to engineer a solution; The network had to find the answers.  I followed method of transfer-learning from lecture X (link) that is the basis of Universal Langage Model Fine-tuning for Text Classification (ULMFiT) (link).

1. Pretrain a language model (LM) with a large corpus of text
2. Fine-tune that LM on the text data for the competition
3. Train a classifier on top of the back-bone LM

# Building a Language Model
A language model (LM) embeds the structure of the lanugage into the neural network.  Whatever you select as the architecture of this model becomese the "backbone" for the classifer you will evenutally use.  I used the AWD_LSTM model with QRNN units as defined in fast.ai.  
```python
config = awd_lstm_lm_config.copy()
config['qrnn'] = True
config['n_hid'] = 2304
learn = language_model_learner(data, AWD_LSTM, drop_mult=dropmult, pretrained=False, config=config)
```
To train this LM, you use the prior information in the sequence of tokens to predict the next token.  Often you have one token per word, but sometimes you can have sub-word units and you are predicting the next sub-word unit in a sequence.  However you tokenize your data, you need a corpus for training the LM and you want the vocabulary of that corpus to map onto your classifier training set as best as possible so that you don't have words that are outside your stated vocabulary.  

# Buliding a Classifier
With a backbone defined, you now change the top of the model and can either create a classifier or a regression head.  For the classifier, I had a label for all my data. Yet, the classes were imbalanced.  I used Synthetic Minority Oversampling Technique (SMOTE) to balance the classes and that got me to 50/50 for the two cases.  I have found from other projects that this can help the mdoel train much better than feeding in the imbalanced cases.  
For the regression, I only had data for the "humorous" tweets.  I filled in the data for the non-humours tweets with zeros since I wanted them to get a low rating.  I did not use SMOTE on this case since I wanted a real-valued output.

# Selecting the LM Training Corpus
This process was iterative.  I made a first attempt, tried to classify the data, then went back and improved my training corpus and continued to refine my LM to improve my model. 

## First Attempt: Wikipeida
I started with Spanish Wikipedia and tried to train an LM from there.  Since I had a pipleine using sentencepiece, I used that for my tests. I took that LM and built a classifier for tweets from the competition (with no fine-tuning) and saw a lot of out of vocabulary tokens. That meant that the vocabulary coverage from Spanish Wikipedia did not overlap well with the Twitter vocabulary.

Even without speaking Spanish, I could see why:
>Jajajajajajaja xque todos me contestan ese motivo, tan obvio es todo (?ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚

>Mi Espalda Me Quiere MatarrrrrrrðŸ˜­ðŸ˜­

>Â¡FELIZ NAVIDAD A TODOS! ðŸŽ„ðŸŽ…ðŸ»

>#FiebreDeViÃ±a Bose tiene cara de muerto, o se cree vampiroðŸŽ§

There are a lot of emoticons!  There is a bunch of slang and other words that would not show up in Wikipedia entries.  It was time to find a new LM corpus. 

## Second Attempt: Spanish Twitter
About a year ago, I had gotten fed up with how Twitter moved around the time sequence of my feed and tried to show me what I might like.  So, I went to look for a command line twitter feed app and found tweepy (link) and adapted that to my needs.  
To collect a corpus for my LM, I pulled any tweet marked as spanish with the terms 'el','su','lo','y' or 'en'.  I excluded re-tweets and put that all into a file.  I ran the script over three 4-hour periods and got 475k tweets to train on.  
When I used this new LM to train my classifier, I had less out of vocab words and I was able to use the emoticons as part of my classification.  Now I was ready to refine the classifier.


# Setting the Seed
In working on some code for the core library, I had found setting the random seed (link stas?) to be very helpful. I got results that were identical so I could test out the code changes I was making. In talking with Piotr on the forums, he suggested using the seed as a hyper-parameter.  With 


***

(include?, separate posts?  just talk about SP and how it works or is used?)
## Sentencepice tokenization to sub-word units
Sentencepice tokenizes into sub-word units (link) and as with all tokenizers, you pick a vocab size cutoff.  When you parse into words, there is often a long-tail of infrequent words in your training set that you excluded from your vocabulary.  

With sentencepiece, you break words up into sub-word units all the way down to single characters.  This means that you could build up a word from the sub-word units and have more coverage and less out of vocabulary words.  

# Lessons Learned:

link to KevinBird blog about the same: http://kevinbird15.com/2019/06/11/Haha-2019-Lessons-Learned.html

Lessons: 
* You learn a lot when pushed by a compeition.  You are critical of your process and you look into all the details to extract and edge.
* The community is a huge help. Put your code out there, people give feedback and want to assist you and help you out.  You get lots of suggestions when you have an example out there.
* Writing up your results and consolatating your code gives you ideas about what makes sense to try/do next and to improve on your model.  When I moved the code into a clean repo and started to document what I had done, I realized I had missed the fine-tuning stage. 
* GIT is your friend.  I came to love it and to use it to see what I had changed/used at each stage of the compitition and then revert back to points where things had run better in my final results.  It really did help me to stay organized during development. 
* Pre-training and transfer learning is a big deal - it works and is worth your time to figure it out in NLP.  You can embbed a lot of information in the network with pre-training.  The winner used pre-trained BERT which is a huge amount of information embedded into the model. 
* Controlling the random seed and using it as a hyperparameter.  Great idea/suggestion.  It really did help me to stay sane and have results that I could replicate when I wanted to go back and re-build a model.  I set the random seed and got the exact same results. 



***
Outline:
1. Saw challenge on forum/Twitter - sounded cool and fun.

Random Seed control / Random seed as a hyper parameter

2. Scraped data from Twitter to use for LM training from scratch
?    2a. Already had SP pipeline for Wiki103 from scratch.  Used again with BPE model for sub-word units
3. Trained model and built classifer - got decent result better than NBSVM
4. Moved over/wrote up results and realized I had _missed_ the fine-tuning step.  So added that back in.

4. Reinfed the model with
    4a. Increased dropout and WD on LM fit to improve generalization
    4b. Added label smoothing to loss function - allowed me to unfreeze all for classifier and fit
    4c. Finetune of the LM on the classification text data (particular task)
    4d. started with 100 random seed init, same validation set for control
    4e. Added cross-validation of results for ensemble - does it work in different validation sub-sets?
        4e i. Run this 5-fold over 20 random seeds (100 tests, but average over each seed.)
    
Tried but did not work:
* Ensemble Forward and Backward classification.  backward was consistently worse, so averaging just reduced the power of the classifier
* Mixup - did mixup after the embedding step.  But that did not seem to help much with the output results.
~~* Started on UDA implemetnation of NMT - but did not get working in time.~~

    

#  Advantages of a Competition Environment

After my first few submissions, I was in first place and became complacent.  I had posted to the forums and had gotten some feedback but had not heard much.  Then, a week before the compeition closed, I got passed by a large margin. What is going on here?  I started to dig in more and it was that competition that really drove me to learn more, try more and do more than I had done for the month before.  

There are several places on the Fast.ai forums where people talk about learning from doing competitions.  I was suspicous of that, but after this experience I can see what they mean.  Having a deadline and having competitors and collaborators really drives you to understand and do more than you would on your own and with your own time. 

# Building a Lanauge Model

The most important things I learned, it was that the quality of the pre-trained Langugae Model (LM) defines your ability to classify well.  An LM is the backbone of of your classifer or regresion model - so you have to get it right.

When you think about an LM, think about predicing the next token in a sequence.  That token can be a word, it can be a sub-word unit, it can be an n-gram of words.  But in the end, you are taking the information up to time t and predicting the next token at time t+1.  I toyed around with several kinds of LMs, but in the end chose the AWD_LSTM to train and apply to this problem.

# Spanish Twitter

When training and LM, you want to use data that is broad and deep in your area of application.  Typically for English, people use the Wikipedia corpus.  When processing this corpus and reducing the data down to a vocabulary of 60,000 words, you find that you drop a lot of less common words and phrases.  If your classification model looks like your training corpus you are all set. If it does not, you are going to get a lot of words that are out-of-vocab (OOV).

The problem with OOV is that you will label those as unknown (in my case, `xxunk` is the token that gets put in there.)  This can be a big gotcha for the LM and lead to false information.  Take the extreme example - all your words are OOV. Now you are going to get `xxunk xxunk xxunk...` which means that no matter what you think you see in a sequence you know the next term you are going to see - `xxunk`!!

To aleivate this problem I did two things
1.  Build a corpus of words from Twitter by sampling the data on a live stream over 3 4-hour sessions.
1.  Use Sentencepiece to break words into sub-word units so that I got a lot of "parts" of words and had less OOV terms when I saw things in my test set.

# Fine Tuning

Spanish twitter is pretty broad.  But for the HAHA compeitition, there is a particular group of terms that we encounter.  So, we can "fine-tune" our LM with the data.  We can take the text part of the train *and test* data and combine it into our fine-tune corpus. Then we can train our LM on that data and get a final LM which can best predict our next-word in the text we expect to see.  For this model, we got about 31% accuracy when predicting the next token.

# Build the classifier or regression model
