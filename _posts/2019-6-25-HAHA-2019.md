---
layout: post
title: HAHA 2019 Competition
---

When this showed up in the [Fastai forum](https://forums.fast.ai/t/nlp-challenge-project/44153), I had to give it a shot.

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.png)<!-- .element height="50%" width="50%" -->

I have been tinkering around with sequence models of various kinds for a few years now and challenge was a great way to refine my knowledge and understanding of their application.  From the posts in the forum, the game plan seemed straight forward;  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

I came in 3rd in classification and 2nd in the regression task.  The top three teams all used some version of a pre-trained lanauge model (LM), a new "head" some kind of ensemble to pick a model for the test set.  

# The Fast.ai Forum Community 

#  Advantages of a Competition Environment

After my first few submissions, I was in first place and became complacent.  I had posted to the forums and had gotten some feedback but had not heard much.  Then, a week before the compeition closed, I got passed by a large margin. What is going on here?  I started to dig in more and it was that competition that really drove me to learn more, try more and do more than I had done for the month before.  

There are several places on the Fast.ai forums where people talk about learning from doing competitions.  I was suspicous of that, but after this experience I can see what they mean.  Having a deadline and having competitors and collaborators really drives you to understand and do more than you would on your own and with your own time. 

# Building a Lanauge Model

The most important things I learned, it was that the quality of the pre-trained Langugae Model (LM) defines your ability to classify well.  An LM is the backbone of of your classifer or regresion model - so you have to get it right.

When you think about an LM, think about predicing the next token in a sequence.  That token can be a word, it can be a sub-word unit, it can be an n-gram of words.  But in the end, you are taking the information up to time $t$ and predicting the next token at time $t+1$.  I toyed around with several kinds of LMs, but in the end chose the AWD_LSTM to train and apply to this problem.

# Spanish Twitter

When training and LM, you want to use data that is broad and deep in your area of application.  Typically for English, people use the Wikipedia corpus.  When processing this corpus and reducing the data down to a vocabulary of 60,000 words, you find that you drop a lot of less common words and phrases.  If your classification model looks like your training corpus you are all set. If it does not, you are going to get a lot of words that are out-of-vocab (OOV).

The problem with OOV is that you will label those as unknown (in my case, `xxunk` is the token that gets put in there.)  This can be a big gotcha for the LM and lead to false information.  Take the extreme example - all your words are OOV. Now you are going to get `xxunk xxunk xxunk...` which means that no matter what you think you see in a sequence you know the next term you are going to see - `xxunk`!!

To aleivate this problem I did two things
1.  Build a corpus of words from Twitter by sampling the data on a live stream over 3 4-hour sessions.
1.  Use Sentencepiece to break words into sub-word units so that I got a lot of "parts" of words and had less OOV terms when I saw things in my test set.

# Fine Tuning

Spanish twitter is pretty broad.  But for the HAHA compeitition, there is a particular group of terms that we encounter.  So, we can "fine-tune" our LM with the data.  We can take the text part of the train *and test* data and combine it into our fine-tune corpus. Then we can train our LM on that data and get a final LM which can best predict our next-word in the text we expect to see.  For this model, we got about 31% accuracy when predicting the next token.

# Build the classifier or regression model
