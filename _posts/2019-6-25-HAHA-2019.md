---
layout: post
title: HAHA 2019 Competition
---

When this image of a tweet showed up in the [Fastai forum](https://forums.fast.ai/t/nlp-challenge-project/44153), I had to give it a shot.

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.png)<!-- .element height="50%" width="50%" -->

I have been tinkering around with sequence models of various kinds for a few years now and challenge was a great way to refine my knowledge and understanding of their application.  From the posts in the forum, the game plan seemed straight forward;  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

I came in 3rd in classification and 2nd in the regression task.  The top three teams all used some version of a pre-trained lanauge model (LM), a new "head" some kind of ensemble to pick a model for the test set.  

# The Fast.ai Forum Community 

For almost a year now, I have been looking around on the fast.ai forums, helping to answer qustions submitting pull requests (PRs) to the core library.  I started with makeing contributions to the documentation working my way up from there.  Starting with documentation was a great way to learn about git, the PR process and the fast.ai style.  The `text` part of the library seemed particularly interesting to me so I dove in deep there.  

I started by replicating the Wiki103 pre-training to IMDB classification pipeline.  I did it all from scratch and swapped out the SpaCy tokenizer for sentencepiece.  From there, I had a full process baseline and developed an understanding of how the parts all worked.  As has been said many times in the forums, you don't learn from watching the videos and reading the notebooks - you learn from building it all yourself.  

# No habla espa&ntilde;ol 

With some baseline understanding behind me and the details of the compeition set out, I started to look at the data.  I don't speak Spanish but I viewed this as an advantage.  Since I could not engineer features I had to apply the process and believe that the process would sort out the details. 

link to KevinBird blog about the same: http://kevinbird15.com/2019/06/11/Haha-2019-Lessons-Learned.html

Lessons: 
* You learn a lot when pushed by a compeition.  You are critical of your process and you look into all the details to extract and edge.
* The community is a huge help. Put your code out there, people give feedback and want to assist you and help you out.  You get lots of suggestions when you have an example out there.
* Writing up your results and consolatating your code gives you ideas about what makes sense to try/do next and to improve on your model.  When I moved the code into a clean repo and started to document what I had done, I realized I had missed the fine-tuning stage. 
* GIT is your friend.  I came to love it and to use it to see what I had changed/used at each stage of the compitition and then revert back to points where things had run better in my final results.  It really did help me to stay organized during development. 
* Pre-training and transfer learning is a big deal - it works and is worth your time to figure it out in NLP.  You can embbed a lot of information in the network with pre-training.  The winner used pre-trained BERT which is a huge amount of information embedded into the model. 
* Controlling the random seed and using it as a hyperparameter.  Great idea/suggestion.  It really did help me to stay sane and have results that I could replicate when I wanted to go back and re-build a model.  I set the random seed and got the exact same results. 




Outline:
1. Saw challenge on forum/Twitter - sounded cool and fun.

Random Seed control / Random seed as a hyper parameter

2. Scraped data from Twitter to use for LM training from scratch
?    2a. Already had SP pipeline for Wiki103 from scratch.  Used again with BPE model for sub-word units
3. Trained model and built classifer - got decent result better than NBSVM
4. Moved over/wrote up results and realized I had _missed_ the fine-tuning step.  So added that back in.

4. Reinfed the model with
    4a. Increased dropout and WD on LM fit to improve generalization
    4b. Added label smoothing to loss function - allowed me to unfreeze all for classifier and fit
    4c. Finetune of the LM on the classification text data (particular task)
    4d. started with 100 random seed init, same validation set for control
    4e. Added cross-validation of results for ensemble - does it work in different validation sub-sets?
        4e i. Run this 5-fold over 20 random seeds (100 tests, but average over each seed.)
    
Tried but did not work:
* Ensemble Forward and Backward classification.  backward was consistently worse, so averaging just reduced the power of the classifier
* Mixup - did mixup after the embedding step.  But that did not seem to help much with the output results.
~~* Started on UDA implemetnation of NMT - but did not get working in time.~~

    


When this image of a tweet showed up in the [Fastai forum](https://forums.fast.ai/t/nlp-challenge-project/44153), I had to give it a shot.

![_config.yml]({{ site.baseurl }}/images/nlp_challenge_tweet.png)<!-- .element height="50%" width="50%" -->

I have been tinkering around with sequence models of various kinds for a few years now and challenge was a great way to refine my knowledge and understanding of their application.  From the posts in the forum, the game plan seemed straight forward;  Establish the baseline for Naive Bayes, then beat it with ULMFiT.

I came in 3rd in classification and 2nd in the regression task.  The top three teams all used some version of a pre-trained lanauge model (LM), a new "head" some kind of ensemble to pick a model for the test set.  

# The Fast.ai Forum Community 

#  Advantages of a Competition Environment

After my first few submissions, I was in first place and became complacent.  I had posted to the forums and had gotten some feedback but had not heard much.  Then, a week before the compeition closed, I got passed by a large margin. What is going on here?  I started to dig in more and it was that competition that really drove me to learn more, try more and do more than I had done for the month before.  

There are several places on the Fast.ai forums where people talk about learning from doing competitions.  I was suspicous of that, but after this experience I can see what they mean.  Having a deadline and having competitors and collaborators really drives you to understand and do more than you would on your own and with your own time. 

# Building a Lanauge Model

The most important things I learned, it was that the quality of the pre-trained Langugae Model (LM) defines your ability to classify well.  An LM is the backbone of of your classifer or regresion model - so you have to get it right.

When you think about an LM, think about predicing the next token in a sequence.  That token can be a word, it can be a sub-word unit, it can be an n-gram of words.  But in the end, you are taking the information up to time t and predicting the next token at time t+1.  I toyed around with several kinds of LMs, but in the end chose the AWD_LSTM to train and apply to this problem.

# Spanish Twitter

When training and LM, you want to use data that is broad and deep in your area of application.  Typically for English, people use the Wikipedia corpus.  When processing this corpus and reducing the data down to a vocabulary of 60,000 words, you find that you drop a lot of less common words and phrases.  If your classification model looks like your training corpus you are all set. If it does not, you are going to get a lot of words that are out-of-vocab (OOV).

The problem with OOV is that you will label those as unknown (in my case, `xxunk` is the token that gets put in there.)  This can be a big gotcha for the LM and lead to false information.  Take the extreme example - all your words are OOV. Now you are going to get `xxunk xxunk xxunk...` which means that no matter what you think you see in a sequence you know the next term you are going to see - `xxunk`!!

To aleivate this problem I did two things
1.  Build a corpus of words from Twitter by sampling the data on a live stream over 3 4-hour sessions.
1.  Use Sentencepiece to break words into sub-word units so that I got a lot of "parts" of words and had less OOV terms when I saw things in my test set.

# Fine Tuning

Spanish twitter is pretty broad.  But for the HAHA compeitition, there is a particular group of terms that we encounter.  So, we can "fine-tune" our LM with the data.  We can take the text part of the train *and test* data and combine it into our fine-tune corpus. Then we can train our LM on that data and get a final LM which can best predict our next-word in the text we expect to see.  For this model, we got about 31% accuracy when predicting the next token.

# Build the classifier or regression model
